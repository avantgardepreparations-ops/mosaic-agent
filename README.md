# ğŸ¤– Mosaic Agent - Interface de Codage IA CentralisÃ©e

Une interface web moderne et responsive pour centraliser et gÃ©rer les meilleurs outils open source d'intelligence artificielle pour le dÃ©veloppement. OptimisÃ© pour **MacBook Pro 2015 (macOS 12.7.6)** avec des recommandations de compatibilitÃ© dÃ©taillÃ©es.

![Interface Preview](docs/preview.png)

## âœ¨ FonctionnalitÃ©s

- ğŸ›ï¸ **Tableau de bord centralisÃ©** pour gÃ©rer tous vos outils IA
- ğŸ”§ **Interface de configuration** pour chaque outil
- ğŸ“Š **Monitoring en temps rÃ©el** des services actifs
- ğŸ–¥ï¸ **CompatibilitÃ© MacBook Pro 2015** avec notes d'optimisation
- ğŸ“š **Documentation intÃ©grÃ©e** avec exemples d'utilisation
- ğŸ³ **Support Docker** pour isolation des environnements
- ğŸŒ **Interface responsive** avec Tailwind CSS

## ğŸ› ï¸ Outils IA IntÃ©grÃ©s

| Outil | Type | CompatibilitÃ© Mac | Description |
|-------|------|-------------------|-------------|
| **Ollama** | LLM Local | âœ… Compatible | ExÃ©cute LLaMA, Mistral localement avec quantification |
| **Whisper.cpp** | Speech-to-Text | âœ… Excellent | Transcription audio optimisÃ©e CPU |
| **ChromaDB** | Base Vectorielle | âœ… Excellent | Stockage et recherche d'embeddings |
| **LangChain** | Framework Agents | âœ… Excellent | ChaÃ®nage de prompts et agents IA |
| **JupyterLab** | IDE Interactif | âœ… Excellent | DÃ©veloppement Python interactif |
| **FAISS** | Recherche SimilaritÃ© | âœ… Compatible | Recherche vectorielle rapide (CPU) |
| **Stable Diffusion** | Text-to-Image | âš ï¸ LimitÃ© | GÃ©nÃ©ration d'images (lent sur CPU) |
| **Text Generation WebUI** | Interface LLM | âš ï¸ LimitÃ© | Interface avancÃ©e pour LLMs |
| **Docker** | Conteneurisation | âœ… Excellent | Isolation des environnements |

### ğŸ“ LÃ©gende CompatibilitÃ©
- âœ… **Excellent/Compatible** : Fonctionne parfaitement sur MacBook Pro 2015
- âš ï¸ **LimitÃ©** : Fonctionnel mais performances limitÃ©es, cloud recommandÃ©

## ğŸš€ Installation Rapide

### PrÃ©requis
- **macOS 12.7.6+**
- **Python 3.8+**
- **8 Go RAM** minimum
- **Homebrew** installÃ©

### 1. Cloner le Repository
```bash
git clone https://github.com/avantgardepreparations-ops/mosaic-agent.git
cd mosaic-agent
```

### 2. Installer les DÃ©pendances Backend
```bash
# Installer les dÃ©pendances Python de base
pip install -r requirements.txt

# Optionnel: installer les outils IA (selon vos besoins)
pip install chromadb langchain faiss-cpu openai-whisper jupyterlab
```

### 3. Installer les Outils SystÃ¨me
```bash
# Ollama (recommandÃ©)
brew install ollama

# Docker (optionnel mais recommandÃ©)
brew install --cask docker

# Whisper.cpp (optionnel)
brew install whisper-cpp
```

### 4. Lancer l'Application
```bash
# 1. DÃ©marrer le backend
python app.py

# 2. Ouvrir l'interface web
open index.html
# ou naviguer vers http://localhost:5000 (si vous servez via Flask)
```

## ğŸ“– Guide d'Utilisation

### DÃ©marrage des Services

1. **Lancer le Backend** : `python app.py`
2. **Ouvrir l'Interface** : Ouvrir `index.html` dans votre navigateur
3. **Activer les Outils** : Cliquer sur "DÃ©marrer" pour chaque outil souhaitÃ©

### Configuration OptimisÃ©e MacBook Pro 2015

#### Pour Ollama (LLMs)
```bash
# TÃ©lÃ©charger un modÃ¨le quantifiÃ© optimisÃ©
ollama pull llama2:7b-chat-q4_0

# ModÃ¨les recommandÃ©s pour 8GB RAM
ollama pull tinyllama:1.1b
ollama pull mistral:7b-instruct-q4_0
```

#### Pour Stable Diffusion (si utilisÃ©)
```python
# Utiliser la quantification pour Ã©conomiser la RAM
from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float32,  # Utiliser float32 sur CPU
    safety_checker=None,  # DÃ©sactiver pour Ã©conomiser la RAM
)
pipe.enable_attention_slicing()  # Optimisation mÃ©moire
```

## ğŸ”§ Configuration AvancÃ©e

### Variables d'Environnement
CrÃ©er un fichier `.env`:
```bash
OLLAMA_URL=http://localhost:11434
CHROMADB_URL=http://localhost:8000
JUPYTER_PORT=8888
WHISPER_MODEL_PATH=./models/ggml-base.bin
MODELS_DIR=./models
```

### Docker Compose (Optionnel)
```yaml
version: '3.8'
services:
  mosaic-agent:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./models:/app/models
    environment:
      - FLASK_ENV=production
  
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
```

## ğŸ“Š API Endpoints

### Gestion des Outils
- `GET /api/tools/status` - Statut de tous les outils
- `POST /api/tools/{tool}/start` - DÃ©marrer un outil
- `POST /api/tools/{tool}/stop` - ArrÃªter un outil

### Ollama Integration
- `GET /api/ollama/models` - Liste des modÃ¨les
- `POST /api/ollama/generate` - GÃ©nÃ©rer du texte

### SystÃ¨me
- `GET /api/system/info` - Informations systÃ¨me
- `GET /api/health` - VÃ©rification de santÃ©
- `GET /api/install-guide` - Guide d'installation

## ğŸ¯ Recommandations MacBook Pro 2015

### âœ… Outils RecommandÃ©s en Local
- **Ollama** avec modÃ¨les 2-4B quantifiÃ©s
- **Whisper** pour transcription
- **ChromaDB/FAISS** pour bases vectorielles
- **LangChain** pour agents
- **JupyterLab** pour dÃ©veloppement

### âš ï¸ Outils NÃ©cessitant le Cloud
- **Stable Diffusion** (gÃ©nÃ©ration d'images)
- **Fine-tuning de LLMs** (entraÃ®nement)
- **ModÃ¨les 7B+** non quantifiÃ©s

### ğŸ”§ Optimisations RecommandÃ©es
1. **ModÃ¨les quantifiÃ©s** : Utilisez GGUF int4/int8
2. **Taille modÃ¨le** : 2-4B paramÃ¨tres maximum
3. **RAM** : Fermez les applications inutiles
4. **Stockage** : 10GB+ libres pour les modÃ¨les

## ğŸ“ Structure du Projet

```
mosaic-agent/
â”œâ”€â”€ index.html          # Interface web principale
â”œâ”€â”€ app.js              # JavaScript frontend
â”œâ”€â”€ app.py              # Backend Flask
â”œâ”€â”€ requirements.txt    # DÃ©pendances Python
â”œâ”€â”€ README.md           # Documentation
â”œâ”€â”€ docs/               # Documentation supplÃ©mentaire
â”œâ”€â”€ models/             # ModÃ¨les IA (crÃ©Ã© automatiquement)
â””â”€â”€ docker-compose.yml  # Configuration Docker (optionnel)
```

## ğŸ” DÃ©pannage

### ProblÃ¨mes Courants

**Ollama ne dÃ©marre pas**
```bash
# VÃ©rifier l'installation
ollama --version

# RedÃ©marrer le service
brew services restart ollama
```

**Erreur de mÃ©moire avec Stable Diffusion**
```python
# Utiliser des optimisations mÃ©moire
pipe.enable_attention_slicing()
pipe.enable_sequential_cpu_offload()
```

**Port dÃ©jÃ  utilisÃ©**
```bash
# Trouver le processus utilisant le port
lsof -i :5000

# Tuer le processus
kill -9 <PID>
```

## ğŸ¤ Contribution

1. Fork le repository
2. CrÃ©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit vos changements (`git commit -m 'Ajouter nouvelle fonctionnalitÃ©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. CrÃ©er une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ™ Remerciements

- [Ollama](https://ollama.ai/) pour l'interface LLM locale
- [OpenAI Whisper](https://github.com/openai/whisper) pour la transcription
- [ChromaDB](https://www.trychroma.com/) pour la base vectorielle
- [LangChain](https://langchain.com/) pour le framework agents
- [Tailwind CSS](https://tailwindcss.com/) pour le design

---

**DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© IA open source**
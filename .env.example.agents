# API Configuration for DistributionAgent
# Copy this file to .env and add your actual API tokens

# Ollama (Local LLM Server)
OLLAMA_URL=http://localhost:11434

# HuggingFace Inference API
HUGGINGFACE_API_TOKEN=your_huggingface_token_here

# Together AI
TOGETHER_API_TOKEN=your_together_token_here

# Replicate
REPLICATE_API_TOKEN=your_replicate_token_here

# OpenRouter
OPENROUTER_API_TOKEN=your_openrouter_token_here

# Groq
GROQ_API_TOKEN=your_groq_token_here

# Anyscale
ANYSCALE_API_TOKEN=your_anyscale_token_here

# Optional: Custom model configurations
# DEFAULT_MODEL=llama3
# DEFAULT_TEMPERATURE=0.7
# DEFAULT_MAX_TOKENS=200
# REQUEST_TIMEOUT=30000
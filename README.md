# ü§ñ Mosaic Agent - Interface de Codage IA Centralis√©e

## ‚úÖ VERSION STABLE ET OP√âRATIONNELLE

**Cette version sur la branche `main` est la version stable et op√©rationnelle de Mosaic Agent.**

Une interface web moderne et responsive pour centraliser et g√©rer les meilleurs outils open source d'intelligence artificielle pour le d√©veloppement. Optimis√© pour **MacBook Pro 2015 (macOS 12.7.6)** avec des recommandations de compatibilit√© d√©taill√©es.

![Interface Preview](docs/preview.png)

## ‚ú® Fonctionnalit√©s

- üéõÔ∏è **Tableau de bord centralis√©** pour g√©rer tous vos outils IA
- üîß **Interface de configuration** pour chaque outil
- üìä **Monitoring en temps r√©el** des services actifs
- üñ•Ô∏è **Compatibilit√© MacBook Pro 2015** avec notes d'optimisation
- üìö **Documentation int√©gr√©e** avec exemples d'utilisation
- üê≥ **Support Docker** pour isolation des environnements
- üåê **Interface responsive** avec Tailwind CSS

## üõ†Ô∏è Outils IA Int√©gr√©s

| Outil | Type | Compatibilit√© Mac | Description |
|-------|------|-------------------|-------------|
| **Ollama** | LLM Local | ‚úÖ Compatible | Ex√©cute LLaMA, Mistral localement avec quantification |
| **Whisper.cpp** | Speech-to-Text | ‚úÖ Excellent | Transcription audio optimis√©e CPU |
| **ChromaDB** | Base Vectorielle | ‚úÖ Excellent | Stockage et recherche d'embeddings |
| **LangChain** | Framework Agents | ‚úÖ Excellent | Cha√Ænage de prompts et agents IA |
| **JupyterLab** | IDE Interactif | ‚úÖ Excellent | D√©veloppement Python interactif |
| **FAISS** | Recherche Similarit√© | ‚úÖ Compatible | Recherche vectorielle rapide (CPU) |
| **Stable Diffusion** | Text-to-Image | ‚ö†Ô∏è Limit√© | G√©n√©ration d'images (lent sur CPU) |
| **Text Generation WebUI** | Interface LLM | ‚ö†Ô∏è Limit√© | Interface avanc√©e pour LLMs |
| **Docker** | Conteneurisation | ‚úÖ Excellent | Isolation des environnements |

### üìù L√©gende Compatibilit√©
- ‚úÖ **Excellent/Compatible** : Fonctionne parfaitement sur MacBook Pro 2015
- ‚ö†Ô∏è **Limit√©** : Fonctionnel mais performances limit√©es, cloud recommand√©

## üöÄ Installation Rapide

### Pr√©requis
- **macOS 12.7.6+**
- **Python 3.8+**
- **8 Go RAM** minimum
- **Homebrew** install√©

### 1. Cloner le Repository
```bash
git clone https://github.com/avantgardepreparations-ops/mosaic-agent.git
cd mosaic-agent
```

### 2. Installer les D√©pendances Backend
```bash
# Installer les d√©pendances Python de base
pip install -r requirements.txt

# Optionnel: installer les outils IA (selon vos besoins)
pip install chromadb langchain faiss-cpu openai-whisper jupyterlab
```

### 3. Installer les Outils Syst√®me
```bash
# Ollama (recommand√©)
brew install ollama

# Docker (optionnel mais recommand√©)
brew install --cask docker

# Whisper.cpp (optionnel)
brew install whisper-cpp
```

### 4. Lancer l'Application
```bash
# 1. D√©marrer le backend
python app.py

# 2. Ouvrir l'interface web
open index.html
# ou naviguer vers http://localhost:5000 (si vous servez via Flask)
```

## üìñ Guide d'Utilisation

### D√©marrage des Services

1. **Lancer le Backend** : `python app.py`
2. **Ouvrir l'Interface** : Ouvrir `index.html` dans votre navigateur
3. **Activer les Outils** : Cliquer sur "D√©marrer" pour chaque outil souhait√©

### Configuration Optimis√©e MacBook Pro 2015

#### Pour Ollama (LLMs)
```bash
# T√©l√©charger un mod√®le quantifi√© optimis√©
ollama pull llama2:7b-chat-q4_0

# Mod√®les recommand√©s pour 8GB RAM
ollama pull tinyllama:1.1b
ollama pull mistral:7b-instruct-q4_0
```

#### Pour Stable Diffusion (si utilis√©)
```python
# Utiliser la quantification pour √©conomiser la RAM
from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float32,  # Utiliser float32 sur CPU
    safety_checker=None,  # D√©sactiver pour √©conomiser la RAM
)
pipe.enable_attention_slicing()  # Optimisation m√©moire
```

## üîß Configuration Avanc√©e

### Variables d'Environnement
Cr√©er un fichier `.env`:
```bash
OLLAMA_URL=http://localhost:11434
CHROMADB_URL=http://localhost:8000
JUPYTER_PORT=8888
WHISPER_MODEL_PATH=./models/ggml-base.bin
MODELS_DIR=./models
```

### Docker Compose (Optionnel)
```yaml
version: '3.8'
services:
  mosaic-agent:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./models:/app/models
    environment:
      - FLASK_ENV=production
  
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
```

## üìä API Endpoints

### Gestion des Outils
- `GET /api/tools/status` - Statut de tous les outils
- `POST /api/tools/{tool}/start` - D√©marrer un outil
- `POST /api/tools/{tool}/stop` - Arr√™ter un outil

### Ollama Integration
- `GET /api/ollama/models` - Liste des mod√®les
- `POST /api/ollama/generate` - G√©n√©rer du texte

### Syst√®me
- `GET /api/system/info` - Informations syst√®me
- `GET /api/health` - V√©rification de sant√©
- `GET /api/install-guide` - Guide d'installation

## üéØ Recommandations MacBook Pro 2015

### ‚úÖ Outils Recommand√©s en Local
- **Ollama** avec mod√®les 2-4B quantifi√©s
- **Whisper** pour transcription
- **ChromaDB/FAISS** pour bases vectorielles
- **LangChain** pour agents
- **JupyterLab** pour d√©veloppement

### ‚ö†Ô∏è Outils N√©cessitant le Cloud
- **Stable Diffusion** (g√©n√©ration d'images)
- **Fine-tuning de LLMs** (entra√Ænement)
- **Mod√®les 7B+** non quantifi√©s

### üîß Optimisations Recommand√©es
1. **Mod√®les quantifi√©s** : Utilisez GGUF int4/int8
2. **Taille mod√®le** : 2-4B param√®tres maximum
3. **RAM** : Fermez les applications inutiles
4. **Stockage** : 10GB+ libres pour les mod√®les

## üìÅ Structure du Projet

```
mosaic-agent/
‚îú‚îÄ‚îÄ index.html          # Interface web principale
‚îú‚îÄ‚îÄ app.js              # JavaScript frontend
‚îú‚îÄ‚îÄ app.py              # Backend Flask
‚îú‚îÄ‚îÄ requirements.txt    # D√©pendances Python
‚îú‚îÄ‚îÄ README.md           # Documentation principale
‚îú‚îÄ‚îÄ main/               # üÜï Syst√®me Multi-Agent ACTIF (workflow multi-agent op√©rationnel)
‚îú‚îÄ‚îÄ docs/               # Documentation suppl√©mentaire
‚îú‚îÄ‚îÄ frontend/           # Composants frontend React
‚îú‚îÄ‚îÄ security/           # Modules de s√©curit√©
‚îú‚îÄ‚îÄ tests/              # Tests principaux
‚îú‚îÄ‚îÄ models/             # Mod√®les IA (cr√©√© automatiquement)
‚îú‚îÄ‚îÄ archive/            # Contenu exp√©rimental archiv√© (NON op√©rationnel)
‚îî‚îÄ‚îÄ docker-compose.yml  # Configuration Docker (optionnel)
```

### üöÄ Nouveau: Syst√®me Multi-Agent dans main/

Le dossier `main/` contient maintenant le **syst√®me multi-agent op√©rationnel** avec:
- **Workflow complet** d'orchestration d'agents IA
- **Documentation int√©gr√©e** et scripts de test fonctionnels
- **S√©paration stricte** avec MOSAICMIND pr√©serv√©e
- **Version stable** pr√™te pour l'utilisation

```bash
# Tester le syst√®me multi-agent
cd main/
npm install
npm run test-agents
```

## üèÅ Cha√Æne Multi‚ÄëAgents & Agent Final

### Pipeline Multi-Agents (√âtapes 1‚Üí9)
1. Prompt initial utilisateur  
2. Affinement 1  
3. G√©n√©rations parall√®les  
4. R√©colte  
5. V√©rification faisabilit√©  
6. Affinement 2  
7. Assemblage  
8. Innovation  
9. **V√©rification Finale (Agent Final)** ‚úÖ

### Agent Final
Produit un rapport consolid√©: m√©triques d'am√©lioration, √©tat de structure, extrait de code final, prochaines √©tapes.

**Endpoint de test:**
```bash
curl http://localhost:5000/api/agent/final/test | jq
```

**Exemple de r√©ponse:**
```json
{
  "run_id": "...",
  "stage": "FINAL_REPORT",
  "report": {
    "summary": "Succ√®s",
    "variant_count": 2,
    "improvement_ratio": 0.29
  }
}
```

### Fichiers ajout√©s
- `agents/agent_final.py` - Classe FinalAgent principale
- `docs/agent-final-analysis.md` - Analyse fonctionnelle & technique
- `docs/screenshots/agent-final-overview.png` - Vue d'ensemble
- `tests/test_agent_final.py` - Tests unitaires

### Screenshot
![Agent Final Overview](docs/screenshots/agent-final-overview.png)

### TODO (Futurs)
- Sandbox Docker (ex√©cution isol√©e)
- Orchestrateur (Temporal / Celery / BullMQ)
- Benchmarks automatiques
- Diff AST + signature rapport
- Couverture tests >85%

### EN (Short Placeholder)
Final Agent consolidates multi-agent pipeline outputs into a delivery report (validation + metrics). Future work: sandbox, orchestration, benchmarks.

## üîç D√©pannage

### Probl√®mes Courants

**Ollama ne d√©marre pas**
```bash
# V√©rifier l'installation
ollama --version

# Red√©marrer le service
brew services restart ollama
```

**Erreur de m√©moire avec Stable Diffusion**
```python
# Utiliser des optimisations m√©moire
pipe.enable_attention_slicing()
pipe.enable_sequential_cpu_offload()
```

**Port d√©j√† utilis√©**
```bash
# Trouver le processus utilisant le port
lsof -i :5000

# Tuer le processus
kill -9 <PID>
```

## ü§ù Contribution

1. Fork le repository
2. Cr√©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit vos changements (`git commit -m 'Ajouter nouvelle fonctionnalit√©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Cr√©er une Pull Request

## üìÅ Contenu Archiv√©

Le dossier `archive/` contient des versions exp√©rimentales et du contenu non op√©rationnel :
- Syst√®mes multi-agents exp√©rimentaux
- Impl√©mentations alternatives (CrewAI, FastAPI)
- Infrastructures backend exp√©rimentales

‚ö†Ô∏è **Attention** : Le contenu archiv√© n'est pas op√©rationnel et est conserv√© uniquement pour r√©f√©rence. Consultez `archive/README.md` pour plus de d√©tails.

## üìÑ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üôè Remerciements

- [Ollama](https://ollama.ai/) pour l'interface LLM locale
- [OpenAI Whisper](https://github.com/openai/whisper) pour la transcription
- [ChromaDB](https://www.trychroma.com/) pour la base vectorielle
- [LangChain](https://langchain.com/) pour le framework agents
- [Tailwind CSS](https://tailwindcss.com/) pour le design

---

**D√©velopp√© avec ‚ù§Ô∏è pour la communaut√© IA open source**
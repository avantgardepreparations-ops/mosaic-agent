# Guide d'Installation et de DÃ©ploiement - Mosaic Agent

## ðŸŽ¯ AperÃ§u

Mosaic Agent est une interface web locale qui centralise et intÃ¨gre les meilleurs outils d'IA open source pour crÃ©er une IA personnelle Ã©volutive. OptimisÃ© spÃ©cifiquement pour **MacBook Pro 2015 (macOS 12.7.6, 8GB RAM, CPU Intel)**.

## ðŸ“‹ PrÃ©requis SystÃ¨me

### Configuration minimale
- **OS**: macOS 12.0+ (optimisÃ© pour 12.7.6)
- **RAM**: 8GB (4GB disponibles recommandÃ©s)
- **CPU**: Intel Core i5 ou supÃ©rieur
- **Stockage**: 50GB libres (pour modÃ¨les IA)
- **Connexion**: Internet pour tÃ©lÃ©chargement initial

### VÃ©rifications prÃ©alables
```bash
# VÃ©rifier la version macOS
sw_vers

# VÃ©rifier l'espace disque disponible
df -h

# VÃ©rifier la RAM
system_profiler SPHardwareDataType | grep "Memory:"
```

## ðŸ› ï¸ Installation

### 1. Installation des outils de base

#### Homebrew (gestionnaire de paquets)
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

#### Python 3.11+
```bash
brew install python@3.11
brew install git
```

#### Xcode Command Line Tools (si nÃ©cessaire)
```bash
xcode-select --install
```

### 2. Installation de Mosaic Agent

#### Cloner le projet
```bash
git clone https://github.com/avantgardepreparations-ops/mosaic-agent.git
cd mosaic-agent
```

#### CrÃ©er un environnement virtuel Python
```bash
python3 -m venv venv
source venv/bin/activate
```

#### Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 3. Installation des outils IA

#### Ollama (Serveur LLM local)
```bash
curl -fsSL https://ollama.ai/install.sh | sh

# TÃ©lÃ©charger un modÃ¨le recommandÃ© pour votre Mac
ollama pull llama2:7b-chat-q4_0    # 4GB - RecommandÃ©
ollama pull tinyllama:1.1b-chat    # 1.2GB - Ultra-rapide pour tests
```

#### Whisper.cpp (Transcription audio)
```bash
git clone https://github.com/ggerganov/whisper.cpp
cd whisper.cpp
make

# TÃ©lÃ©charger le modÃ¨le base (recommandÃ© pour votre Mac)
bash ./models/download-ggml-model.sh base
cd ..
```

#### JupyterLab (IDE IA)
```bash
pip install jupyterlab
pip install ipywidgets  # Pour les widgets interactifs
```

#### ChromaDB (Base vectorielle)
```bash
pip install chromadb
```

#### LangChain (Framework d'agents)
```bash
pip install langchain
pip install langchain-community
```

#### Docker (Optionnel mais recommandÃ©)
```bash
# TÃ©lÃ©charger et installer Docker Desktop pour Mac
# https://www.docker.com/products/docker-desktop/
```

## ðŸš€ DÃ©marrage

### 1. Lancer Mosaic Agent
```bash
cd mosaic-agent
source venv/bin/activate
python app.py
```

### 2. Ouvrir l'interface
Ouvrir dans votre navigateur : **http://localhost:5000**

### 3. VÃ©rification des services
L'interface affichera automatiquement l'Ã©tat de chaque service :
- âœ… **Vert** : Service fonctionnel sur votre Mac
- âš ï¸ **Orange** : Service limitÃ©, optimisation requise
- âŒ **Rouge** : Service non compatible, cloud recommandÃ©

## âš™ï¸ Configuration et Optimisation

### Optimisation mÃ©moire pour 8GB RAM

#### 1. Configuration systÃ¨me
```bash
# Augmenter le swap (optionnel)
sudo sysctl vm.swapusage

# Fermer les applications non-essentielles
osascript -e 'quit app "Safari"'
osascript -e 'quit app "Chrome"'
```

#### 2. Configuration Ollama optimisÃ©e
```bash
# Variables d'environnement pour optimiser Ollama
export OLLAMA_NUM_PARALLEL=1        # Un seul modÃ¨le Ã  la fois
export OLLAMA_MAX_LOADED_MODELS=1   # Limite de modÃ¨les chargÃ©s
export OLLAMA_FLASH_ATTENTION=1     # Optimisation attention
```

#### 3. ModÃ¨les recommandÃ©s par taille
```bash
# Ultra-lÃ©ger (1-2GB) - Tests rapides
ollama pull tinyllama:1.1b-chat
ollama pull phi:2.7b-chat-q4_K_M

# LÃ©ger (3-4GB) - Usage quotidien
ollama pull llama2:7b-chat-q4_0
ollama pull mistral:7b-instruct-q4_K_M
ollama pull codellama:7b-code-q4_0

# Ã‰viter (>6GB) - Trop lourd pour votre Mac
# llama2:13b, llama2:70b, etc.
```

### Configuration des ports par dÃ©faut
```
Mosaic Agent:     http://localhost:5000
Ollama API:       http://localhost:11434
JupyterLab:       http://localhost:8888
ChromaDB:         http://localhost:8000
```

## ðŸ“Š Utilisation

### Interface principale
1. **Tableau de bord** : Vue d'ensemble des services et performances
2. **Outils IA** : Gestion de chaque application
3. **Configuration** : Optimisations et paramÃ¨tres
4. **Documentation** : Guides et exemples

### DÃ©marrage rapide
1. Cliquer sur "DÃ©marrer" pour Ollama
2. Attendre le chargement du modÃ¨le (~30s)
3. Tester avec l'API ou JupyterLab
4. Ajouter d'autres services selon les besoins

### Exemples d'utilisation

#### Test rapide Ollama
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama2:7b-chat-q4_0",
  "prompt": "Ã‰cris une fonction Python pour trier une liste",
  "stream": false
}'
```

#### IntÃ©gration Python
```python
import requests

def query_ollama(prompt, model="llama2:7b-chat-q4_0"):
    response = requests.post('http://localhost:11434/api/generate', 
                           json={'model': model, 'prompt': prompt, 'stream': False})
    return response.json()['response']

# Test
result = query_ollama("Explique les algorithmes de tri en Python")
print(result)
```

## ðŸ”§ DÃ©pannage

### ProblÃ¨mes courants

#### Ollama ne dÃ©marre pas
```bash
# VÃ©rifier si le port est utilisÃ©
lsof -i :11434

# RedÃ©marrer Ollama
pkill ollama
ollama serve
```

#### Manque de mÃ©moire
```bash
# VÃ©rifier l'usage RAM
top -l 1 | grep "PhysMem:"

# LibÃ©rer la mÃ©moire
sudo purge

# Utiliser un modÃ¨le plus petit
ollama pull tinyllama:1.1b-chat
```

#### Erreur Python/pip
```bash
# RecrÃ©er l'environnement virtuel
rm -rf venv
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Limitations connues sur MacBook Pro 2015

| Outil | Limite | Solution |
|-------|--------|----------|
| **Stable Diffusion** | 15-30min par image | Utiliser services cloud (Replicate, HF Spaces) |
| **Fine-tuning LLM** | Impossible sans GPU | Cloud (Google Colab, RunPod) |
| **ModÃ¨les >7B** | Trop lent | Rester sur modÃ¨les 7B quantifiÃ©s max |
| **Embeddings lourds** | RAM limitÃ©e | Utiliser des modÃ¨les d'embeddings lÃ©gers |

## ðŸŽ›ï¸ Configuration avancÃ©e

### Automatisation du dÃ©marrage
```bash
# CrÃ©er un script de dÃ©marrage
cat > ~/start-mosaic.sh << 'EOF'
#!/bin/bash
cd ~/mosaic-agent
source venv/bin/activate
python app.py
EOF

chmod +x ~/start-mosaic.sh
```

### Variables d'environnement
```bash
# Ajouter Ã  ~/.zshrc ou ~/.bash_profile
export MOSAIC_AGENT_PORT=5000
export OLLAMA_HOST=localhost:11434
export JUPYTER_PORT=8888
```

### Service systemd/launchd (optionnel)
Pour macOS, crÃ©er un LaunchAgent pour dÃ©marrage automatique.

## ðŸ“š Ressources supplÃ©mentaires

### Documentation officielle
- [Ollama](https://ollama.ai/docs)
- [Whisper.cpp](https://github.com/ggerganov/whisper.cpp)
- [LangChain](https://python.langchain.com/)
- [ChromaDB](https://docs.trychroma.com/)

### ModÃ¨les recommandÃ©s
- [Ollama Library](https://ollama.ai/library)
- [Hugging Face](https://huggingface.co/models)

### Optimisations CPU Intel
- [Intel AI Tools](https://software.intel.com/content/www/us/en/develop/tools/ai-tools.html)
- [MKLDNN optimizations](https://github.com/intel/mkl-dnn)

## ðŸ†˜ Support

### Logs et diagnostic
```bash
# Logs Mosaic Agent
tail -f mosaic-agent.log

# Logs Ollama
ollama logs

# Ã‰tat systÃ¨me
system_profiler SPSoftwareDataType
```

### CommunautÃ©
- GitHub Issues pour bugs et features
- Discussions pour questions gÃ©nÃ©rales
- Wiki pour documentation communautaire

---

**Note**: Cette installation est optimisÃ©e pour votre configuration spÃ©cifique (MacBook Pro 2015). Pour d'autres configurations, ajustez les paramÃ¨tres de mÃ©moire et de CPU en consÃ©quence.